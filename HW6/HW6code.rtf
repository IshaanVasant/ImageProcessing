{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red183\green111\blue179;
\red202\green202\blue202;\red70\green137\blue204;\red212\green214\blue154;\red140\green211\blue254;\red67\green192\blue160;
\red89\green138\blue67;\red167\green197\blue152;\red194\green126\blue101;\red238\green46\blue56;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c0\c0;\cssrgb\c77255\c52549\c75294;
\cssrgb\c83137\c83137\c83137;\cssrgb\c33725\c61176\c83922;\cssrgb\c86275\c86275\c66667;\cssrgb\c61176\c86275\c99608;\cssrgb\c30588\c78824\c69020;
\cssrgb\c41569\c60000\c33333;\cssrgb\c70980\c80784\c65882;\cssrgb\c80784\c56863\c47059;\cssrgb\c95686\c27843\c27843;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl360\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 import\strokec5  keras\
\strokec4 from\strokec5  keras.datasets \strokec4 import\strokec5  mnist\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\strokec4 import\strokec5  saab_compact \strokec4 as\strokec5  saab\
\strokec4 from\strokec5  sklearn.model_selection \strokec4 import\strokec5  train_test_split\
\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \strokec6 def\strokec5  \strokec7 get_data_for_class\strokec5 (\strokec8 images\strokec5 , \strokec8 labels\strokec5 , \strokec8 cls\strokec5 ):\
\'a0\'a0\'a0\'a0\strokec4 if\strokec5  \strokec9 type\strokec5 (\strokec6 cls\strokec5 )==\strokec9 list\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0idx=np.zeros(labels.shape, \strokec8 dtype\strokec5 =\strokec9 bool\strokec5 )\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec4 for\strokec5  c \strokec6 in\strokec5  \strokec6 cls\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0idx=np.logical_or(idx, labels==c)\
\'a0\'a0\'a0\'a0\strokec4 else\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0idx=(labels==\strokec6 cls\strokec5 )\
\'a0\'a0\'a0\'a0\strokec4 return\strokec5  images[idx], labels[idx]\
\
\strokec6 def\strokec5  \strokec7 import_data\strokec5 (\strokec8 use_classes\strokec5 ):\
\'a0\'a0\'a0\'a0(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\
\'a0\'a0\'a0\'a0\strokec10 # X_train, X_test, y_train, y_test = train_test_split(train_images1, train_labels1, train_size=60000, random_state=42)\strokec5 \
\'a0\'a0\'a0\'a0\strokec10 # train_images=X_train\strokec5 \
\'a0\'a0\'a0\'a0\strokec10 # train_labels=y_train\strokec5 \
\'a0\'a0\'a0\'a0train_images=train_images.reshape(-\strokec11 1\strokec5 ,\strokec11 28\strokec5 ,\strokec11 28\strokec5 ,\strokec11 1\strokec5 )\
\'a0\'a0\'a0\'a0test_images=test_images.reshape(-\strokec11 1\strokec5 ,\strokec11 28\strokec5 ,\strokec11 28\strokec5 ,\strokec11 1\strokec5 )\
\'a0\'a0\'a0\'a0train_images=train_images/\strokec11 255\strokec5 .\
\'a0\'a0\'a0\'a0test_images=test_images/\strokec11 255\strokec5 .\
\
\'a0\'a0\'a0\'a0train_images=np.float32(train_images)\
\'a0\'a0\'a0\'a0test_images=np.float32(test_images)\
\
\'a0\'a0\'a0\'a0\strokec7 print\strokec5 (\strokec12 'initiali dtype: '\strokec5 , train_images.dtype)\
\'a0\'a0\'a0\'a0\strokec10 # print(train_images.shape) # 60000*28*28*1\strokec5 \
\
\'a0\'a0\'a0\'a0\strokec10 # zeropadding\strokec5 \
\'a0\'a0\'a0\'a0train_images=np.pad(train_images, ((\strokec11 0\strokec5 ,\strokec11 0\strokec5 ),(\strokec11 2\strokec5 ,\strokec11 2\strokec5 ),(\strokec11 2\strokec5 ,\strokec11 2\strokec5 ),(\strokec11 0\strokec5 ,\strokec11 0\strokec5 )), \strokec8 mode\strokec5 =\strokec12 'constant'\strokec5 )\
\'a0\'a0\'a0\'a0test_images=np.pad(test_images,  ((\strokec11 0\strokec5 ,\strokec11 0\strokec5 ),(\strokec11 2\strokec5 ,\strokec11 2\strokec5 ),(\strokec11 2\strokec5 ,\strokec11 2\strokec5 ),(\strokec11 0\strokec5 ,\strokec11 0\strokec5 )), \strokec8 mode\strokec5 =\strokec12 'constant'\strokec5 )\
\'a0\'a0\'a0\'a0\strokec10 # print(train_images.shape) # 60000*32*32*1\strokec5 \
\
\'a0\'a0\'a0\'a0\strokec4 if\strokec5  use_classes!=\strokec12 '0-9'\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0class_list=saab.parse_list_string(use_classes)\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0train_images, train_labels=get_data_for_class(train_images, train_labels, class_list)\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0test_images, test_labels=get_data_for_class(test_images, test_labels, class_list)\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # print(class_list)\strokec5 \
\'a0\'a0\'a0\'a0\strokec4 else\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0class_list=[\strokec11 0\strokec5 ,\strokec11 1\strokec5 ,\strokec11 2\strokec5 ,\strokec11 3\strokec5 ,\strokec11 4\strokec5 ,\strokec11 5\strokec5 ,\strokec11 6\strokec5 ,\strokec11 7\strokec5 ,\strokec11 8\strokec5 ,\strokec11 9\strokec5 ]\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\
\'a0\'a0\'a0\'a0\strokec4 return\strokec5  train_images, train_labels, test_images, test_labels, class_list\
\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \
\strokec4 from\strokec5  tensorflow.python.platform \strokec4 import\strokec5  flags\
\strokec4 import\strokec5  pickle\
\strokec4 import\strokec5  data3\
\strokec4 import\strokec5  saab3\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\
\strokec6 def\strokec5  \strokec7 del_all_flags\strokec5 (\strokec8 FLAGS\strokec5 ):\
    \strokec7 print\strokec5 (\strokec12 "Type of flags is"\strokec5 ,\strokec9 type\strokec5 (FLAGS))\
    flags_dict = FLAGS._flags()    \
    keys_list = [keys \strokec4 for\strokec5  keys \strokec6 in\strokec5  flags_dict]    \
    \strokec4 for\strokec5  keys \strokec6 in\strokec5  keys_list:\
        FLAGS.\strokec7 __delattr__\strokec5 (keys)\
\
del_all_flags(flags.FLAGS)\
\
flags.DEFINE_string(\strokec12 "output_path"\strokec5 , \strokec6 None\strokec5 , \strokec12 "The output dir to save params"\strokec5 )\
flags.DEFINE_string(\strokec12 "use_classes"\strokec5 , \strokec12 "0-9"\strokec5 , \strokec12 "Supported format: 0,1,5-9"\strokec5 )\
flags.DEFINE_string(\strokec12 "kernel_sizes"\strokec5 , \strokec12 "5,5"\strokec5 , \strokec12 "Kernels size for each stage. Format: '3,3'"\strokec5 )\
flags.DEFINE_string(\strokec12 "num_kernels"\strokec5 , \strokec12 "5,15"\strokec5 , \strokec12 "Num of kernels for each stage. Format: '4,10'"\strokec5 )\
flags.DEFINE_float(\strokec12 "energy_percent"\strokec5 , \strokec6 None\strokec5 , \strokec12 "Energy to be preserved in each stage"\strokec5 )\
flags.DEFINE_integer(\strokec12 "use_num_images"\strokec5 , \strokec11 10000\strokec5 , \strokec12 "Num of images used for training"\strokec5 )\
FLAGS = flags.FLAGS\
\
\strokec6 def\strokec5  \strokec7 convolution\strokec5 (\strokec8 i\strokec5 ,\strokec8 j\strokec5 ,\strokec8 kernel\strokec5 ,\strokec8 image\strokec5 ):\
    pix = \strokec11 0.0\strokec13 ;\strokec5 \
    \strokec4 for\strokec5  k \strokec6 in\strokec5  \strokec7 range\strokec5 (i,i+\strokec11 5\strokec5 ):\
        \strokec4 for\strokec5  l \strokec6 in\strokec5  \strokec7 range\strokec5 (j,j+\strokec11 5\strokec5 ):\
            pix+= (image[k][l] * kernel[k-i][l-j])\
    \
    \strokec4 return\strokec5  pix\strokec13 ;\strokec5 \
\
\strokec6 def\strokec5  \strokec7 main\strokec5 ():\
\'a0\'a0\'a0\'a0\strokec10 # read data\strokec5 \
    \
                \
               \
        \
    train_images, train_labels, test_images, test_labels, class_list = data3.import_data(FLAGS.use_classes)\
    \strokec7 print\strokec5 (\strokec12 'Training image size:'\strokec5 , train_images.shape)\
    \strokec7 print\strokec5 (\strokec12 'Testing_image size:'\strokec5 , test_images.shape)\
    \
\
    kernel_sizes=saab3.parse_list_string(FLAGS.kernel_sizes)\
    \strokec4 if\strokec5  FLAGS.num_kernels:\
    \'a0\'a0\'a0\'a0num_kernels=saab3.parse_list_string(FLAGS.num_kernels)\
    \strokec4 else\strokec5 :\
    \'a0\'a0\'a0\'a0num_kernels=\strokec6 None\strokec5 \
    energy_percent=FLAGS.energy_percent\
    use_num_images=FLAGS.use_num_images\
    \strokec7 print\strokec5 (\strokec12 'Parameters:'\strokec5 )\
    \strokec7 print\strokec5 (\strokec12 'use_classes:'\strokec5 , class_list)\
    \strokec7 print\strokec5 (\strokec12 'Kernel_sizes:'\strokec5 , kernel_sizes)\
    \strokec7 print\strokec5 (\strokec12 'Number_kernels:'\strokec5 , num_kernels)\
    \strokec7 print\strokec5 (\strokec12 'Energy_percent:'\strokec5 , energy_percent)\
    \strokec7 print\strokec5 (\strokec12 'Number_use_images:'\strokec5 , use_num_images)\
\
    pca_params=saab3.multi_Saab_transform(train_images, train_labels,\
    \'a0\'a0\'a0\'a0                 \strokec8 kernel_sizes\strokec5 =kernel_sizes,\
    \'a0\'a0\'a0\'a0                 \strokec8 num_kernels\strokec5 =num_kernels,\
    \'a0\'a0\'a0\'a0                 \strokec8 energy_percent\strokec5 =energy_percent,\
    \'a0\'a0\'a0\'a0                 \strokec8 use_num_images\strokec5 =use_num_images,\
    \'a0\'a0\'a0\'a0                 \strokec8 use_classes\strokec5 =class_list)\
    \strokec10 # save data\strokec5 \
    fw=\strokec7 open\strokec5 (\strokec12 'pca_params2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )    \
    pickle.dump(pca_params, fw)    \
    fw.close()\
\
    \strokec10 # load data\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'pca_params2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    data1=pickle.load(fr)\
    \strokec7 print\strokec5 (data1)\
    fr.close()\
\
\strokec4 if\strokec5  \strokec8 __name__\strokec5  == \strokec12 '__main__'\strokec5 :\
\'a0\'a0\'a0\'a0main()\
\
\
\strokec4 import\strokec5  pickle\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\strokec4 import\strokec5  data3\
\strokec4 import\strokec5  saab3\
\strokec4 import\strokec5  keras\
\strokec4 import\strokec5  sklearn\
\
\strokec6 def\strokec5  \strokec7 main\strokec5 ():\
\'a0\'a0\'a0\'a0\strokec10 # load data\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'pca_params2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    pca_params=pickle.load(fr)\
    fr.close()\
\
\'a0\'a0\'a0\'a0\strokec10 # read data\strokec5 \
    train_images, train_labels, test_images, test_labels, class_list = data3.import_data(\strokec12 "0-9"\strokec5 )\
    \strokec7 print\strokec5 (\strokec12 'Training image size:'\strokec5 , train_images.shape)\
    \strokec7 print\strokec5 (\strokec12 'Testing_image size:'\strokec5 , test_images.shape)\
\'a0\'a0\'a0\'a0\
\'a0\'a0\'a0\'a0\strokec10 # Training\strokec5 \
    \strokec7 print\strokec5 (\strokec12 '--------Training--------'\strokec5 )\
    featu=[]\
    \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec11 10\strokec5 ):\
        feature=saab3.initialize(train_images[i*\strokec11 6000\strokec5 :(i+\strokec11 1\strokec5 )*\strokec11 6000\strokec5 ], pca_params)\
        feature=feature.reshape(feature.shape[\strokec11 0\strokec5 ],-\strokec11 1\strokec5 )\
        featu.append(feature)\
    featu=np.array(featu).reshape(\strokec11 60000\strokec5 ,-\strokec11 1\strokec5 )\
    \
    \strokec7 print\strokec5 (\strokec12 "S4 shape:"\strokec5 , featu.shape)\
    \strokec7 print\strokec5 (\strokec12 '--------Finish Feature Extraction subnet--------'\strokec5 )\
    feat=\{\}\
    feat[\strokec12 'feature'\strokec5 ]=featu\
\'a0\'a0\'a0\'a0\
\'a0\'a0\'a0\'a0\strokec10 # save data\strokec5 \
    fw=\strokec7 open\strokec5 (\strokec12 'feat2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )    \
    pickle.dump(feat, fw)    \
    fw.close()\
\
\strokec4 if\strokec5  \strokec8 __name__\strokec5  == \strokec12 '__main__'\strokec5 :\
\'a0\'a0\'a0\'a0main()\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \
\strokec4 import\strokec5  data3\
\strokec4 import\strokec5  pickle\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\strokec4 import\strokec5  sklearn\
\strokec4 import\strokec5  cv2\
\strokec4 import\strokec5  keras\
\strokec4 from\strokec5  sklearn.decomposition \strokec4 import\strokec5  PCA\
\strokec4 from\strokec5  sklearn.cluster \strokec4 import\strokec5  KMeans\
\strokec4 from\strokec5  numpy \strokec4 import\strokec5  linalg \strokec4 as\strokec5  LA\
\strokec4 import\strokec5  matplotlib.pyplot \strokec4 as\strokec5  plt\
\strokec4 from\strokec5  sklearn.metrics.pairwise \strokec4 import\strokec5  euclidean_distances\
\
\strokec6 def\strokec5  \strokec7 main\strokec5 ():\
\'a0\'a0\'a0\'a0\strokec10 # load data\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'pca_params2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    pca_params=pickle.load(fr, \strokec8 encoding\strokec5 =\strokec12 'latin1'\strokec5 )\
    fr.close()\
\
\'a0\'a0\'a0\'a0\strokec10 # read data\strokec5 \
    train_images, train_labels, test_images, test_labels, class_list = data3.import_data(\strokec12 "0-9"\strokec5 )\
    \strokec7 print\strokec5 (\strokec12 'Training image size:'\strokec5 , train_images.shape)\
    \strokec7 print\strokec5 (\strokec12 'Testing_image size:'\strokec5 , test_images.shape)\
\
    \strokec10 # load feature\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'feat2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    feat=pickle.load(fr, \strokec8 encoding\strokec5 =\strokec12 'latin1'\strokec5 )\
    fr.close()\
    feature=feat[\strokec12 'feature'\strokec5 ]\
    \strokec7 print\strokec5 (\strokec12 "S4 shape:"\strokec5 , feature.shape)\
    \strokec7 print\strokec5 (\strokec12 '--------Finish Feature Extraction subnet--------'\strokec5 )\
\
\'a0\'a0\'a0\'a0\strokec10 # feature normalization\strokec5 \
    \strokec10 #std_var=(np.std(feature, axis=0)).reshape(1,-1)\strokec5 \
    \strokec10 #feature=feature/std_var\strokec5 \
\
    num_clusters=[\strokec11 120\strokec5 , \strokec11 84\strokec5 , \strokec11 10\strokec5 ]\
    use_classes=\strokec11 10\strokec5 \
    weights=\{\}\
    bias=\{\}\
    \strokec4 for\strokec5  k \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec7 len\strokec5 (num_clusters)):\
        \strokec4 if\strokec5  k!=\strokec7 len\strokec5 (num_clusters)-\strokec11 1\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # Kmeans_Mixed_Class (too slow for CIFAR, changed into Fixed Class)\strokec5 \
            kmeans=KMeans(\strokec8 n_clusters\strokec5 =num_clusters[k]).fit(feature)\
            pred_labels=kmeans.labels_\
            num_clas=np.zeros((num_clusters[k],use_classes))\
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (num_clusters[k]):\
                \strokec4 for\strokec5  t \strokec6 in\strokec5  \strokec7 range\strokec5 (use_classes):\
                    \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                        \strokec4 if\strokec5  pred_labels[j]==i \strokec6 and\strokec5  train_labels[j]==t:\
                            num_clas[i,t]+=\strokec11 1\strokec5 \
            acc_train=np.sum(np.amax(num_clas, \strokec8 axis\strokec5 =\strokec11 1\strokec5 ))/feature.shape[\strokec11 0\strokec5 ]\
            \strokec7 print\strokec5 (k,\strokec12 ' layer Kmean (just ref) training acc is \strokec6 \{\}\strokec12 '\strokec5 .format(acc_train))\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # Compute centroids\strokec5 \
            clus_labels=np.argmax(num_clas, \strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            centroid=np.zeros((num_clusters[k], feature.shape[\strokec11 1\strokec5 ]))\
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (num_clusters[k]):\
                t=\strokec11 0\strokec5 \
                \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                    \strokec4 if\strokec5  pred_labels[j]==i \strokec6 and\strokec5  clus_labels[i]==train_labels[j]:\
                        \strokec4 if\strokec5  t==\strokec11 0\strokec5 :\
                            feature_test=feature[j].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 )\
                        \strokec4 else\strokec5 :\
                            feature_test=np.concatenate((feature_test, feature[j].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 )), \strokec8 axis\strokec5 =\strokec11 0\strokec5 )\
                            t+=\strokec11 1\strokec5 \
                centroid[i]=np.mean(feature_test, \strokec8 axis\strokec5 =\strokec11 0\strokec5 , \strokec8 keepdims\strokec5 =\strokec6 True\strokec5 )\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # Compute one hot vector\strokec5 \
            t=\strokec11 0\strokec5 \
            labels=np.zeros((feature.shape[\strokec11 0\strokec5 ], num_clusters[k]))\
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                \strokec4 if\strokec5  clus_labels[pred_labels[i]]==train_labels[i]:\
                    labels[i,pred_labels[i]]=\strokec11 1\strokec5 \
                \strokec4 else\strokec5 :\
                    distance_assigned=euclidean_distances(feature[i].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 ), centroid[pred_labels[i]].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 ))\
                    cluster_special=[j \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (num_clusters[k]) \strokec4 if\strokec5  clus_labels[j]==train_labels[i]]\
                    distance=np.zeros(\strokec7 len\strokec5 (cluster_special))\
                    \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec7 len\strokec5 (cluster_special)):\
                        distance[j]=euclidean_distances(feature[i].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 ), centroid[cluster_special[j]].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 ))\
                    labels[i, cluster_special[np.argmin(distance)]]=\strokec11 1\strokec5 \
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # least square regression\strokec5 \
            A=np.ones((feature.shape[\strokec11 0\strokec5 ],\strokec11 1\strokec5 ))\
            feature=np.concatenate((A,feature),\strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            weight=np.matmul(LA.pinv(feature),labels)\
            feature=np.matmul(feature,weight)\
            weights[\strokec12 '\strokec6 %d\strokec12  LLSR weight'\strokec5 %k]=weight[\strokec11 1\strokec5 :weight.shape[\strokec11 0\strokec5 ]]\
            bias[\strokec12 '\strokec6 %d\strokec12  LLSR bias'\strokec5 %k]=weight[\strokec11 0\strokec5 ].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 )\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR weight shape:'\strokec5 , weight.shape)\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR output shape:'\strokec5 , feature.shape)\
\
            pred_labels=np.argmax(feature, \strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            num_clas=np.zeros((num_clusters[k],use_classes))\
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (num_clusters[k]):\
                \strokec4 for\strokec5  t \strokec6 in\strokec5  \strokec7 range\strokec5 (use_classes):\
                    \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                        \strokec4 if\strokec5  pred_labels[j]==i \strokec6 and\strokec5  train_labels[j]==t:\
                            num_clas[i,t]+=\strokec11 1\strokec5 \
                            acc_train=np.sum(np.amax(num_clas, \strokec8 axis\strokec5 =\strokec11 1\strokec5 ))/feature.shape[\strokec11 0\strokec5 ]\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR training acc is \strokec6 \{\}\strokec12 '\strokec5 .format(acc_train))\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # Relu\strokec5 \
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 1\strokec5 ]):\
                    \strokec4 if\strokec5  feature[i,j]<\strokec11 0\strokec5 :\
                        feature[i,j]=\strokec11 0\strokec5 \
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # # Double relu\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # for i in range(feature.shape[0]):\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # \'a0\'a0for j in range(feature.shape[1]):\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # \'a0\'a0\'a0\'a0\'a0\'a0if feature[i,j]<0:\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0feature[i,j]=0\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # \'a0\'a0\'a0\'a0\'a0\'a0elif feature[i,j]>1:\strokec5 \
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0feature[i,j]=1\strokec5 \
        \strokec4 else\strokec5 :\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # least square regression\strokec5 \
            labels=keras.utils.to_categorical(train_labels,\strokec11 10\strokec5 )\
            A=np.ones((feature.shape[\strokec11 0\strokec5 ],\strokec11 1\strokec5 ))\
            feature=np.concatenate((A,feature),\strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            weight=np.matmul(LA.pinv(feature),labels)\
            feature=np.matmul(feature,weight)\
            weights[\strokec12 '\strokec6 %d\strokec12  LLSR weight'\strokec5 %k]=weight[\strokec11 1\strokec5 :weight.shape[\strokec11 0\strokec5 ]]\
            bias[\strokec12 '\strokec6 %d\strokec12  LLSR bias'\strokec5 %k]=weight[\strokec11 0\strokec5 ].reshape(\strokec11 1\strokec5 ,-\strokec11 1\strokec5 )\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR weight shape:'\strokec5 , weight.shape)\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR output shape:'\strokec5 , feature.shape)\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\
            pred_labels=np.argmax(feature, \strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            acc_train=sklearn.metrics.accuracy_score(train_labels,pred_labels)\
            \strokec7 print\strokec5 (\strokec12 'training acc is \strokec6 \{\}\strokec12 '\strokec5 .format(acc_train))\
\'a0\'a0\'a0\'a0\strokec10 # save data\strokec5 \
    \
    fw=\strokec7 open\strokec5 (\strokec12 'feature_vector2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )    \
    pickle.dump(feature, fw)    \
    fw.close()\
    fw=\strokec7 open\strokec5 (\strokec12 'llsr_weights2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )    \
    pickle.dump(weights, fw)    \
    fw.close()\
    fw=\strokec7 open\strokec5 (\strokec12 'llsr_bias2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )    \
    pickle.dump(bias, fw)    \
    fw.close()\
\
\strokec4 if\strokec5  \strokec8 __name__\strokec5  == \strokec12 '__main__'\strokec5 :\
\'a0\'a0\'a0\'a0main()\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \strokec4 import\strokec5  data3\
\strokec4 import\strokec5  saab3\
\strokec4 import\strokec5  pickle\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\strokec4 import\strokec5  sklearn\
\strokec4 import\strokec5  cv2\
\strokec4 import\strokec5  keras\
\strokec4 from\strokec5  sklearn.decomposition \strokec4 import\strokec5  PCA\
\strokec4 from\strokec5  sklearn.cluster \strokec4 import\strokec5  KMeans\
\strokec4 from\strokec5  numpy \strokec4 import\strokec5  linalg \strokec4 as\strokec5  LA\
\strokec4 import\strokec5  matplotlib.pyplot \strokec4 as\strokec5  plt\
\strokec4 from\strokec5  sklearn.metrics.pairwise \strokec4 import\strokec5  euclidean_distances\
\
\strokec6 def\strokec5  \strokec7 main\strokec5 ():\
\'a0\'a0\'a0\'a0\strokec10 # load data\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'pca_params2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    pca_params=pickle.load(fr, \strokec8 encoding\strokec5 =\strokec12 'latin1'\strokec5 )\
    fr.close()\
\
    \strokec10 # read data\strokec5 \
    train_images, train_labels, test_images, test_labels, class_list = data3.import_data(\strokec12 "0-9"\strokec5 )\
    \strokec7 print\strokec5 (\strokec12 'Training image size:'\strokec5 , train_images.shape)\
    \strokec7 print\strokec5 (\strokec12 'Testing_image size:'\strokec5 , test_images.shape)\
\
    \strokec10 # testing\strokec5 \
    \strokec7 print\strokec5 (\strokec12 '--------Testing--------'\strokec5 )\
    feature=saab3.initialize(test_images, pca_params)\
    feature=feature.reshape(feature.shape[\strokec11 0\strokec5 ],-\strokec11 1\strokec5 )\
    \strokec7 print\strokec5 (\strokec12 "S4 shape:"\strokec5 , feature.shape)\
    \strokec7 print\strokec5 (\strokec12 '--------Finish Feature Extraction subnet--------'\strokec5 )\
\
\'a0\'a0\'a0\'a0\strokec10 # feature normalization\strokec5 \
\'a0\'a0\'a0\'a0\strokec10 #std_var=(np.std(feature, axis=0)).reshape(1,-1)\strokec5 \
\'a0\'a0\'a0\'a0\strokec10 #feature=feature/std_var\strokec5 \
\'a0\'a0\'a0\'a0\
    num_clusters=[\strokec11 120\strokec5 , \strokec11 84\strokec5 , \strokec11 10\strokec5 ]\
    use_classes=\strokec11 10\strokec5 \
    fr=\strokec7 open\strokec5 (\strokec12 'llsr_weights2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    weights=pickle.load(fr)\
    fr.close()\
    fr=\strokec7 open\strokec5 (\strokec12 'llsr_bias2.pkl'\strokec5 ,\strokec12 'rb'\strokec5 )  \
    biases=pickle.load(fr)\
    fr.close()\
\
    \strokec4 for\strokec5  k \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec7 len\strokec5 (num_clusters)):\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # least square regression\strokec5 \
        weight=weights[\strokec12 '\strokec6 %d\strokec12  LLSR weight'\strokec5 %k]\
        bias=biases[\strokec12 '\strokec6 %d\strokec12  LLSR bias'\strokec5 %k]\
        feature=np.matmul(feature,weight)\
        feature=feature+bias\
        \strokec7 print\strokec5 (k,\strokec12 ' layer LSR weight shape:'\strokec5 , weight.shape)\
        \strokec7 print\strokec5 (k,\strokec12 ' layer LSR bias shape:'\strokec5 , bias.shape)\
        \strokec7 print\strokec5 (k,\strokec12 ' layer LSR output shape:'\strokec5 , feature.shape)\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\
        \strokec4 if\strokec5  k!=\strokec7 len\strokec5 (num_clusters)-\strokec11 1\strokec5 :\
            pred_labels=np.argmax(feature, \strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            num_clas=np.zeros((num_clusters[k],use_classes))\
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (num_clusters[k]):\
                \strokec4 for\strokec5  t \strokec6 in\strokec5  \strokec7 range\strokec5 (use_classes):\
                    \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                        \strokec4 if\strokec5  pred_labels[j]==i \strokec6 and\strokec5  train_labels[j]==t:\
                            num_clas[i,t]+=\strokec11 1\strokec5 \
            acc_train=np.sum(np.amax(num_clas, \strokec8 axis\strokec5 =\strokec11 1\strokec5 ))/feature.shape[\strokec11 0\strokec5 ]\
            \strokec7 print\strokec5 (k,\strokec12 ' layer LSR testing acc is \strokec6 \{\}\strokec12 '\strokec5 .format(acc_train))\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\strokec10 # Relu\strokec5 \
            \strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 0\strokec5 ]):\
                \strokec4 for\strokec5  j \strokec6 in\strokec5  \strokec7 range\strokec5 (feature.shape[\strokec11 1\strokec5 ]):\
                    \strokec4 if\strokec5  feature[i,j]<\strokec11 0\strokec5 :\
                        feature[i,j]=\strokec11 0\strokec5 \
        \strokec4 else\strokec5 :\
            pred_labels=np.argmax(feature, \strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
            acc_train=sklearn.metrics.accuracy_score(test_labels,pred_labels)\
            \strokec7 print\strokec5 (\strokec12 'testing acc is \strokec6 \{\}\strokec12 '\strokec5 .format(acc_train))\
\
    fw=\strokec7 open\strokec5 (\strokec12 'test_features2.pkl'\strokec5 ,\strokec12 'wb'\strokec5 )\
    pickle.dump(feature,fw)\
    fw.close()\
\strokec4 if\strokec5  \strokec8 __name__\strokec5  == \strokec12 '__main__'\strokec5 :\
\'a0\'a0\'a0\'a0main()\
\
\
\
\strokec4 from\strokec5  tensorflow.python.platform \strokec4 import\strokec5  flags\
\strokec4 import\strokec5  pickle\
\strokec4 import\strokec5  data3\
\strokec4 import\strokec5  saab3\
\strokec4 import\strokec5  numpy \strokec4 as\strokec5  np\
\strokec4 from\strokec5  sklearn.decomposition \strokec4 import\strokec5  PCA\
\strokec4 from\strokec5  sklearn.svm \strokec4 import\strokec5  SVC\
\
\strokec6 def\strokec5  \strokec7 del_all_flags\strokec5 (\strokec8 FLAGS\strokec5 ):\
    \strokec7 print\strokec5 (\strokec12 "Type of flags is"\strokec5 ,\strokec9 type\strokec5 (FLAGS))\
    flags_dict = FLAGS._flags()    \
    keys_list = [keys \strokec4 for\strokec5  keys \strokec6 in\strokec5  flags_dict]    \
    \strokec4 for\strokec5  keys \strokec6 in\strokec5  keys_list:\
        FLAGS.\strokec7 __delattr__\strokec5 (keys)\
del_all_flags(flags.FLAGS)        \
\
flags.DEFINE_string(\strokec12 "use_classes"\strokec5 , \strokec12 "0-9"\strokec5 , \strokec12 "Supported format: 0,1,5-9"\strokec5 )\
FLAGS = flags.FLAGS\
\
\
\
\
\
\
feat=[]\
\strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec11 1\strokec5 ,\strokec11 11\strokec5 ):\
    fr=\strokec7 open\strokec5 (\strokec12 'feature_vector\strokec6 %d\strokec12 .pkl'\strokec5 %i,\strokec12 'rb'\strokec5 )  \
    feat1=pickle.load(fr, \strokec8 encoding\strokec5 =\strokec12 'latin1'\strokec5 )\
    \strokec4 if\strokec5  i==\strokec11 1\strokec5 :\
        feat = feat1\
    \strokec4 else\strokec5 :\
        feat = np.concatenate((feat,feat1),\strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
    fr.close()\
\
train_images, train_labels, test_images, test_labels, class_list = data3.import_data(FLAGS.use_classes)\
\
pca = PCA(\strokec8 n_components\strokec5  = \strokec11 10\strokec5 )\
X = pca.fit_transform(feat)\
\
clf = SVC()\
clf.fit(X,train_labels)\
\
training = clf.score(X,train_labels)\
\strokec7 print\strokec5 (\strokec12 "Training accuracy using Ensemble model is: "\strokec5 ,training)\
\
\strokec4 for\strokec5  i \strokec6 in\strokec5  \strokec7 range\strokec5 (\strokec11 1\strokec5 ,\strokec11 11\strokec5 ):\
    fr=\strokec7 open\strokec5 (\strokec12 'test_features\strokec6 %d\strokec12 .pkl'\strokec5 %i,\strokec12 'rb'\strokec5 )  \
    feat1=pickle.load(fr, \strokec8 encoding\strokec5 =\strokec12 'latin1'\strokec5 )\
    \strokec4 if\strokec5  i==\strokec11 1\strokec5 :\
        feat = feat1\
    \strokec4 else\strokec5 :\
        feat = np.concatenate((feat,feat1),\strokec8 axis\strokec5 =\strokec11 1\strokec5 )\
    fr.close()\
X_test = pca.transform(feat)\
test = clf.score(X_test,test_labels)\
\strokec7 print\strokec5 (\strokec12 "Testing accuracy using Ensemble model is: "\strokec5 ,test)\
\
\
\
\
\
\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 \
\
\
}